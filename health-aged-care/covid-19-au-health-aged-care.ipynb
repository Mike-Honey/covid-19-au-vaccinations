{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## covid-19-au-health-aged-care.ipynb\n",
    "\n",
    "Downloads pdf and Word docx files from the sub-pages below a targetted page into a local directory (datadir). Only downloads if local file does not exist.  \n",
    "Reads the text from each local pdf file, extracting key fields. \n",
    "Reads the local Word docx files (dated after a specified date), extracting all table data.\n",
    "\n",
    "Writes the collected data out as an Excel file:\n",
    "- sheet: Treatments - data on treatments scraped from pdf files\n",
    "- sheet: National Snapshot - data from 1st table in Word docx files\n",
    "- sheet: Active Outbreak Summary - data from 2nd table in Word docx files\n",
    "- sheet: Workforce Resources - data from 3rd table in Word docx files\n",
    "- sheet: Vaccinations - data from 4th table in Word docx files\n",
    "- sheet: Regulatory Activities - data from 5th table in Word docx files\n",
    "- sheet: Active Outbreaks - data from 6th table in Word docx files\n",
    "\n",
    "In all sheets, the source_file_name and source_file_date (derived from the file name) are added as columns. Rows are sorted by source_file_date (descending).\n",
    "\n",
    "In the sheets sourced from Word docx tables, the column headers are not promoted so the columns are numbered from 0 instead. This is to avoid a sparse table if column headings change among files.  The column headers will appear repeated for each source file, which any downstream analysis can filter out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import dateutil # pip install python-dateutil\n",
    "from docx import Document\n",
    "import pymupdf\n",
    "import os\n",
    "import pandas\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "\n",
    "site = \"https://www.health.gov.au\"\n",
    "main_url = \"https://www.health.gov.au/resources/collections/covid-19-outbreaks-in-australian-residential-aged-care-facilities\"\n",
    "links_to_check = [\"2025\"]\n",
    "\n",
    "datadir = 'c:/dev/covid-19-au-vaccinations/health-aged-care/'\n",
    "output_filename = datadir + \"health-aged-care.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(pdf_url, local_dir):\n",
    "    # Get the filename from the URL\n",
    "    filename = os.path.basename(pdf_url)\n",
    "    local_path = os.path.join(local_dir, filename)\n",
    "\n",
    "    # Check if the file already exists in the local directory\n",
    "    if not os.path.exists(local_path):\n",
    "        # sleep for a random time before downloading \n",
    "        time.sleep(2 + ( random.randrange( 0, 30) / 10 ) )\n",
    "        # Download the PDF file\n",
    "        response = requests.get(pdf_url)\n",
    "\n",
    "        # Save the PDF file to the local directory\n",
    "        with open(local_path, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Downloaded {filename} to {local_dir}\")\n",
    "    else:\n",
    "        print(f\"{filename} already exists in {local_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_pdf(pdf_file):\n",
    "\n",
    "# open a pdf file, search for the key fields and return them\n",
    "    \n",
    "    with pymupdf.open(pdf_file) as doc:  # open document\n",
    "        text = chr(12).join([page.get_text() for page in doc])\n",
    "\n",
    "        lagevrio_treatment_courses = 0\n",
    "        lagevrio_prescriptions = 0\n",
    "        paxlovid_prescriptions = 0\n",
    "        end_date = ''\n",
    "\n",
    "        # search for: and up to DD MMMM YYYY (allowing for extra spaces around the month)\n",
    "        pattern = r'Lagevrio.*?up\\s+to\\s+(\\d+)(.*?)(\\d{4})'\n",
    "        match = re.search(pattern, text, re.DOTALL)\n",
    "        if match:\n",
    "            # assign result, removing excess whitespace around the month portion\n",
    "            end_date = match.group(1) + ' ' + match.group(2) + ' ' + match.group(3)\n",
    "            # print(f\"DEBUG: End dates: {end_date}\")\n",
    "\n",
    "        # search for: deployed NNN treatment courses of Lagevrio\n",
    "        pattern = r\"deployed\\s*(\\d+(?:,\\d+)?)\\s*treatment\\s+courses\\s+of\\s+Lagevrio\"\n",
    "        match = re.search(pattern, text, re.DOTALL)\n",
    "        if match:\n",
    "            # assign result\n",
    "            lagevrio_treatment_courses = match.group(1)\n",
    "            # print(f\"DEBUG: Text: {text}\")\n",
    "            # print(f\"DEBUG: Lagevrio treatment courses: {lagevrio_treatment_courses}\")\n",
    "\n",
    "        # search for: NNN prescriptions for Lagevrio\n",
    "        pattern = r\"(\\d+(?:,\\d+)?)\\s*prescriptions\\s+for\\s+Lagevrio\"\n",
    "        match = re.search(pattern, text, re.DOTALL)\n",
    "        if match:\n",
    "            # assign result\n",
    "            lagevrio_prescriptions = match.group(1)\n",
    "            # print(f\"DEBUG: Lagevrio prescriptions: {lagevrio_prescriptions}\")\n",
    "\n",
    "        # search for: NNN prescriptions for Paxlovid \n",
    "        pattern = r\"further\\s*(\\d+(?:,\\d+)?)\\s*prescriptions\\s+for\\s+Paxlovid\"\n",
    "        match = re.search(pattern, text, re.DOTALL)\n",
    "        if match:\n",
    "            # assign result \n",
    "            paxlovid_prescriptions = match.group(1)\n",
    "            # print(f\"DEBUG: Paxlovid prescriptions: {paxlovid_prescriptions}\")\n",
    "\n",
    "        return end_date, lagevrio_treatment_courses, lagevrio_prescriptions, paxlovid_prescriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the main source page, make a list of links to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(main_url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find and extract the links with text (wrapped in a span tag) containing with \"COVID-19 outbreaks in Australian residential aged care facilities\"\n",
    "soup_page_select = \"a:has(span:-soup-contains('COVID-19 outbreaks in Australian residential aged care'))\"\n",
    "links_year_pages = [link.get(\"href\") for link in soup.select(soup_page_select)]\n",
    "len ( links_year_pages ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get each sub-page and download it's pdf and Word docx files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# browse through the list of year pages. See if the link URL partially matches the links_to_check list (recent years).\n",
    "link_files_to_download = ['PDF', 'Word']\n",
    "for each_link_year_page_candidate in links_year_pages:\n",
    "    each_link_year_page_list = [each_link_year_page_candidate for sub_string in links_to_check if(sub_string in each_link_year_page_candidate)]\n",
    "    if len(each_link_year_page_list) > 0:\n",
    "        # sleep for a random time before getting the sub-page \n",
    "        time.sleep(1 + ( random.randrange( 0, 20) / 10 ) )\n",
    "        # get and process the sub-page from each qualifying link\n",
    "        each_link_year_page = each_link_year_page_list[0]\n",
    "        year_page_url = site + each_link_year_page\n",
    "        year_page_response = requests.get(year_page_url)\n",
    "        year_page_soup = BeautifulSoup(year_page_response.content, \"html.parser\")\n",
    "\n",
    "        links = [link.get(\"href\") for link in year_page_soup.select(soup_page_select)]\n",
    "\n",
    "        # browse through the list of links. See if the link URL partially matches the links_to_check list (recent years).\n",
    "        for each_link_candidate in links:\n",
    "            each_link_list = [each_link_candidate for sub_string in links_to_check if(sub_string in each_link_candidate)]\n",
    "            if len(each_link_list) > 0:\n",
    "                # sleep for a random time before getting the sub-page \n",
    "                time.sleep(1 + ( random.randrange( 0, 20) / 10 ) )\n",
    "                # get and process the sub-page from each qualifying link\n",
    "                each_link = each_link_list[0]\n",
    "                sub_page_url = site + each_link\n",
    "                sub_page_response = requests.get(sub_page_url)\n",
    "                sub_page_soup = BeautifulSoup(sub_page_response.content, \"html.parser\")\n",
    "\n",
    "                # Find and extract the links to files\n",
    "                for each_link_file_to_download in link_files_to_download:\n",
    "                    soup_select = \"a:has(span:-soup-contains('\" + each_link_file_to_download + \"'))\"\n",
    "                    file_links = [file_links.get(\"href\") for file_links in sub_page_soup.select(soup_select)]\n",
    "\n",
    "                    # downloading the first pdf file link to the local directory (if it doesnt already exist)\n",
    "                    download_pdf(site + file_links[0], datadir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process local files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_source_file_date_for_table_output = datetime.date(2024, 4, 1) # process files dated 1 April 2024 onwards (March 2024 files show a format change)\n",
    "\n",
    "output_treatments_df = pandas.DataFrame(columns=['source_file_name', 'source_file_date', 'end_date', 'lagevrio_courses', 'lagevrio_prescriptions', 'paxlovid_prescriptions'])\n",
    "  \n",
    "# Initialize an empty list to store dataframes from Word tables\n",
    "dfs = [pandas.DataFrame() for _ in range(6)]\n",
    "\n",
    "for file in os.listdir(datadir):\n",
    "    filename = os.fsdecode(file)\n",
    "    # try to derive the source_file_date from the end of the file name\n",
    "    try:\n",
    "        filename_for_source_file_date = filename.split('.')[0]\n",
    "        # handle file names that end with _0, _1 etc\n",
    "        re_does_filename_for_source_file_date_end_in_underscore_number = re.search(r'_\\d$', filename_for_source_file_date)\n",
    "        if re_does_filename_for_source_file_date_end_in_underscore_number is not None:\n",
    "            filename_for_source_file_date = filename_for_source_file_date[:len(filename_for_source_file_date) - 2]\n",
    "        source_file_date_str = ' '.join(filename_for_source_file_date.split('-')[-3:])\n",
    "        source_file_date = dateutil.parser.parse(source_file_date_str, default=datetime.date(2000, 1, 1))\n",
    "    except:\n",
    "        source_file_date = datetime.date(2000, 1, 1)\n",
    "    \n",
    "    # browse through all the local pdf files, gathering the search results into a dataframe for output\n",
    "    if filename.endswith('.pdf'):\n",
    "        pdf_file = datadir + filename\n",
    "        end_date, lagevrio_courses, lagevrio_prescriptions, paxlovid_prescriptions  = extract_data_from_pdf(pdf_file)\n",
    "        # print(f\"DEBUG: End date: {end_date}\")\n",
    "        # print(f\"DEBUG: Lagevrio treatment courses: {lagevrio_courses}\")\n",
    "        # print(f\"DEBUG: Lagevrio prescriptions: {lagevrio_prescriptions}\")\n",
    "        # print(f\"DEBUG: Paxlovid prescriptions: {paxlovid_prescriptions}\")\n",
    "\n",
    "        # construct the output row and add it to the dataframe\n",
    "        output_row = [filename, source_file_date, end_date, lagevrio_courses, lagevrio_prescriptions, paxlovid_prescriptions]\n",
    "        output_treatments_df.loc[len(output_treatments_df.index)] = output_row\n",
    "\n",
    "    # browse through all the local docx files, gathering the tables into dataframes for output\n",
    "    if filename.endswith('.docx') and source_file_date >= start_source_file_date_for_table_output:\n",
    "        # Load the Word document\n",
    "        doc = Document(datadir + filename)\n",
    "        table_counter = -1\n",
    "\n",
    "        # Iterate through each table in the document\n",
    "        for table in doc.tables:\n",
    "            table_counter = table_counter + 1\n",
    "\n",
    "            # Create a DataFrame structure with empty strings, sized by the number of rows and columns in the table\n",
    "            df = [['' for _ in range(len(table.columns))] for _ in range(len(table.rows))]\n",
    "            \n",
    "            # Iterate through each row in the current table\n",
    "            for i, row in enumerate(table.rows):\n",
    "                # Iterate through each cell in the current row\n",
    "                for j, cell in enumerate(row.cells):\n",
    "                    # If the cell has text, store it in the corresponding DataFrame position\n",
    "                    if cell.text:\n",
    "                        df[i][j] = cell.text.replace('\\n',' ')\n",
    "            \n",
    "            # Convert the list of lists (df) to a pandas DataFrame and add it to the tables list\n",
    "            table_df = pandas.DataFrame(df)\n",
    "            table_df['source_file_name'] = filename\n",
    "            table_df['source_file_date'] = source_file_date\n",
    "            dfs[table_counter] = pandas.concat([dfs[table_counter], table_df])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather result dataframes and write out to Excel sheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sort each df by source_file_date desc\n",
    "output_treatments_df = output_treatments_df.sort_values(['source_file_date'], ascending=[False]).reset_index(drop=True)\n",
    "for df_index, each_df in enumerate(dfs.copy()):\n",
    "   each_df.index.name = 'row_index_per_file'\n",
    "   dfs[df_index] = each_df.sort_values(['source_file_date', 'row_index_per_file'], ascending=[False, True]).reset_index()\n",
    "\n",
    "# write all the output...dfs to an Excel file with Sheet names\n",
    "dfs.insert(0,output_treatments_df)\n",
    "\n",
    "writer = pandas.ExcelWriter(output_filename, engine='xlsxwriter')\n",
    "sheet_names = [\"Treatments\", \"National Snapshot\", \"Active Outbreak Summary\", \"Workforce Resources\", \"Vaccinations\",\"Regulatory Activities\",\"Active Outbreaks\"]\n",
    "for df_index, frame in enumerate(dfs):\n",
    "   frame.to_excel(writer, sheet_name = sheet_names[df_index])\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debug dataframe outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_treatments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[1] # National Snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[2] # Active Outbreaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[3] # Workforce Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[4] # Vaccinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[5] # Regulatory Activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[6] # Active Outbreaks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
